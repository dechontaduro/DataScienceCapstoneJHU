---
title: "Capstone - Exploratory Data Analysis"
author: "Juan Carlos González Cardona"
date: "28 de octubre de 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Synopsis
For the capstone of the Data Science Specialization of Coursera and John Hopkins with SwiftKey, i should make app to predict the next word based in a set of documents.

The first step in building a predictive model for text is understanding the distribution and relationship between the words, tokens, and phrases in the text.

I have 3 types of source of documents: news, blogs and twitter, i made this analysis to each type separated because i want that prediction is based at context.

For propuse of reading, i put the code at final of document in appendix.

#Preparing data
```{r libraries, echo=FALSE, warning=FALSE, message=FALSE}
library(tm)
library(RWeka)
library(stringi)
library(slam)
library(wordcloud)
```

```{r variables, echo=FALSE}
proyPath <- "C:\\Users\\juanc\\Dropbox\\DataScience\\Capstone\\ExploratoryDataAnalysis"
urlData <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
fileData <- "C:\\Dev\\R\\TextMining\\Coursera-SwiftKey.zip"
fileTwitter <- "C:\\Dev\\R\\TextMining\\final\\en_US\\en_US.twitter.txt"
fileBlog <- "C:\\Dev\\R\\TextMining\\final\\en_US\\en_US.blogs.txt"
fileNews <- "C:\\Dev\\R\\TextMining\\final\\en_US\\en_US.news.txt"
RData <- "C:\\Dev\\R\\TextMining\\CapstoneExploratory.RData"

setwd(proyPath)
```

##Download data
Data can be download from [data](`r urlData`), only load english files.

```{r download, echo=FALSE, cache=TRUE, warning=FALSE, message=FALSE}
downloadAndUnzip <- function (url, filename, filenameIntoZip){
  if(!file.exists(filename)) { 
    download.file(url, destfile=filename, method="curl") 
  }

  if(!file.exists(filenameIntoZip)) { 
    unzip(filename)
  }
}

readFile <- function(filename){
    con <- file(filename, "r")
    fileobj <- readLines(con)
    close(con)
    return(fileobj)
}



downloadAndUnzip(urlData, fileData, fileTwitter)

news <- readFile(fileNews)
blog <- readFile(fileBlog)
twitter <- readFile(fileTwitter)

```

##Información básica de los documentos
In the next table summary info about three sources. 
(I used package stringi to get basic info from documents because is faster)


```{r fileStats, echo=FALSE, cache=TRUE}

fileStats <- data.frame(File = c("news", "blogs", "twitter")
           , t(sapply(list(news, blog, twitter), stri_stats_general))
           , t(sapply(list(news, blog, twitter), stri_stats_latex))
           , t(sapply(list(nchar(news), nchar(blog), nchar(twitter)), summary))
           )

fileStats <- subset(fileStats, select = -c(LinesNEmpty, CharsNWhite, CharsWord, CharsCmdEnvir
                                          , CharsWhite,  Cmds, Envirs
                                          , X1st.Qu., Median, X3rd.Qu.))

colnames(fileStats) <- c("File", "Lines", "Chars", "Words", "Chars/line Min.", "Mean", "Max.")
```

```{r echo=FALSE}
knitr::kable(fileStats, caption = "File basic statistics")
```

##Sample
The big size of sources is necesary made a sample of them, i take only a 10% of documents per each source. Summary of lines is similar of original source.

```{r fileSample, echo=FALSE, cache=TRUE}

set.seed(1234)
news.sample <- sample(news, size = round(length(news) * 0.1), replace =FALSE)
blog.sample <- sample(blog, size = round(length(blog) * 0.1), replace =FALSE)
twitter.sample <- sample(twitter, size = round(length(twitter) * 0.1), replace =FALSE)

fileStats.sample <- data.frame(File = c("news", "blogs", "twitter")
                        , t(sapply(list(news.sample, blog.sample, twitter.sample), stri_stats_general))
                        , t(sapply(list(news.sample, blog.sample, twitter.sample), stri_stats_latex))
                        , t(sapply(list(nchar(news.sample), nchar(blog.sample), nchar(twitter.sample)), summary))
)

fileStats.sample <- subset(fileStats.sample, select = -c(LinesNEmpty, CharsNWhite, CharsWord, CharsCmdEnvir
                                           , CharsWhite,  Cmds, Envirs
                                           , X1st.Qu., Median, X3rd.Qu.))

colnames(fileStats.sample) <- c("File", "Lines", "Chars", "Words", "Chars/line Min.", "Mean", "Max.")

```

```{r echo=FALSE}
knitr::kable(fileStats.sample, caption = "File sample basic statistics")
```

#Corpus
I create corpus for each source and apply the next transformations:removePunctuation, stripWhitespace, removeNumbers, tolower but i don't remove stopWords because i think that these are important to prediction.

```{r corpus, echo=FALSE, cache=TRUE}
cleanCorpus <- function (corpus){
  corpus <- tm_map(corpus, removePunctuation)
  corpus <- tm_map(corpus, stripWhitespace)
  corpus <- tm_map(corpus, removeNumbers)
  corpus <- tm_map(corpus, content_transformer(tolower))
  #corpus <- tm_map(corpus, stemDocument)
  #corpus <- tm_map(corpus, removeWords, stopwords())
  corpus <- tm_map(corpus, PlainTextDocument)
  return(corpus)
}

news.corpus <- Corpus(VectorSource(news.sample))
blog.corpus <- Corpus(VectorSource(blog.sample))
twitter.corpus <- Corpus(VectorSource(twitter.sample))

news.corpus.clean <- cleanCorpus(news.corpus)
blog.corpus.clean <- cleanCorpus(blog.corpus)
twitter.corpus.clean <- cleanCorpus(twitter.corpus)

```

##Unigrams
I created unigrams from corpus of each source.
I used package slam to extrac frecuency of unigrams because others forms are very expensive in terms of memory.

```{r unigrams, echo=FALSE, cache=TRUE}
news.tdm <-  TermDocumentMatrix(news.corpus.clean)
blog.tdm <- TermDocumentMatrix(blog.corpus.clean)
twitter.tdm <- TermDocumentMatrix(twitter.corpus.clean)

news.tdm.rs <- removeSparseTerms(news.tdm, 0.999)
blog.tdm.rs <- removeSparseTerms(blog.tdm, 0.995)
twitter.tdm.rs <- removeSparseTerms(twitter.tdm, 0.997)

news.wordfrec <- sort(rowSums(as.matrix(news.tdm.rs)), decreasing=TRUE)
blog.wordfrec <- sort(rowSums(as.matrix(blog.tdm.rs)), decreasing=TRUE)
twitter.wordfrec <- sort(rowSums(as.matrix(twitter.tdm.rs)), decreasing=TRUE)
```

Unigrams with low frecuency is predominant.
```{r unigramsHist, echo=FALSE, cache=TRUE}
par(mfrow=c(1,3)) 

news.wordfrec <- row_sums(news.tdm, na.rm = TRUE)
blog.wordfrec <- row_sums(blog.tdm, na.rm = TRUE)
twitter.wordfrec <- row_sums(twitter.tdm, na.rm = TRUE)

hist(news.wordfrec
     , breaks = 800
     , xlim = c(0,100)
     , main = "News term frecuency", xlab = "Term's frecuency")
hist(blog.wordfrec
     , breaks = 2000
     , xlim = c(0,150)
     , main = "Blog term frecuency", xlab = "Term's frecuency")
hist(twitter.wordfrec
     , breaks = 1000
     , xlim = c(0,150)
     , main = "Twitter term frecuency", xlab = "Term's frecuency")

```

At last, i show the most frecuent unigrams. First unigrams are common between sources, but the next are diferent.

```{r unigramsTop, echo=FALSE, cache=TRUE}
par(las=2) # make label text perpendicular to axis
par(mfrow=c(1,3)) 

barplot(news.wordfrec[1:25]
        , names.arg=names(news.wordfrec[1:25])
        , main="News - 25 most frecuent Words"
        , horiz=TRUE
        , cex.names=1.2)
barplot(blog.wordfrec[1:25]
        , names.arg=names(blog.wordfrec[1:25])
        , main="Blog - 25 most frecuent Words"
        , horiz=TRUE
        , cex.names=1.2)
barplot(twitter.wordfrec[1:25]
        , names.arg=names(twitter.wordfrec[1:25])
        , main="Twitter - 25 most frecuent Words"
        , horiz=TRUE
        , cex.names=1.2)
```


##Bigrams
I created bigrams from corpus from each source.

```{r bigrams, echo=FALSE, cache=TRUE}
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))

news.tdm2 <- TermDocumentMatrix(news.corpus.clean, control = list(tokenize = BigramTokenizer))
blog.tdm2 <- TermDocumentMatrix(blog.corpus.clean, control = list(tokenize = BigramTokenizer))
twitter.tdm2 <- TermDocumentMatrix(twitter.corpus.clean, control = list(tokenize = BigramTokenizer))

news.tdm2.rs <- removeSparseTerms(news.tdm2, 0.9995)
blog.tdm2.rs <- removeSparseTerms(blog.tdm2, 0.998)
twitter.tdm2.rs <- removeSparseTerms(twitter.tdm2, 0.998)

news.bigramfrec <- sort(rowSums(as.matrix(news.tdm2.rs)), decreasing=TRUE)
blog.bigramfrec <- sort(rowSums(as.matrix(blog.tdm2.rs)), decreasing=TRUE)
twitter.bigramfrec <- sort(rowSums(as.matrix(twitter.tdm2.rs)), decreasing=TRUE)

```

```{r bigramsHist, echo=FALSE, cache=TRUE}
par(mfrow=c(1,3)) 

#news.bigramfrec <- row_sums(news.tdm2, na.rm = TRUE)
#blog.bigramfrec <- row_sums(blog.tdm2, na.rm = TRUE)
#twitter.bigramfrec <- row_sums(twitter.tdm2, na.rm = TRUE)

hist(news.bigramfrec
     , breaks = 100, xlim = c(0,150)
     , main = "News term frecuency", xlab = "Term's frecuency")
hist(blog.bigramfrec
     , breaks = 150, xlim = c(0,2000)
     , main = "Blog term frecuency", xlab = "Term's frecuency")
hist(twitter.bigramfrec
     , breaks = 50, xlim = c(0,4000)
     , main = "Twitter term frecuency", xlab = "Term's frecuency")

```


```{r bigramsTop, echo=FALSE, cache=TRUE}
par(las=2) # make label text perpendicular to axis
par(mfrow=c(1,3)) 

barplot(news.bigramfrec[1:25]
        , names.arg=names(news.bigramfrec[1:25])
        , main="News - 25 most frecuent bigrams"
        , horiz=TRUE
        , cex.names=1.2)
barplot(blog.bigramfrec[1:25]
        , names.arg=names(blog.bigramfrec[1:25])
        , main="Blog - 25 most frecuent bigrams"
        , horiz=TRUE
        , cex.names=1.2)
barplot(twitter.bigramfrec[1:25]
        , names.arg=names(twitter.bigramfrec[1:25])
        , main="Twitter - 25 most frecuent bigrams"
        , horiz=TRUE
        , cex.names=1.2)
```

##Trigrams
I created trigrams from corpus from each source.

```{r trigrams, echo=FALSE, cache=TRUE}
TrigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))

news.tdm3 <- TermDocumentMatrix(news.corpus.clean, control = list(tokenize = TrigramTokenizer))
blog.tdm3 <- TermDocumentMatrix(blog.corpus.clean, control = list(tokenize = TrigramTokenizer))
twitter.tdm3 <- TermDocumentMatrix(twitter.corpus.clean, control = list(tokenize = TrigramTokenizer))

news.tdm3.rs <- removeSparseTerms(news.tdm3, 0.9995)
blog.tdm3.rs <- removeSparseTerms(blog.tdm3, 0.9995)
twitter.tdm3.rs <- removeSparseTerms(twitter.tdm3, 0.9997)

news.trigramfrec <- sort(rowSums(as.matrix(news.tdm3.rs)), decreasing=TRUE)
blog.trigramfrec <- sort(rowSums(as.matrix(blog.tdm3.rs)), decreasing=TRUE)
twitter.trigramfrec <- sort(rowSums(as.matrix(twitter.tdm3.rs)), decreasing=TRUE)

```

```{r trigramsHist, echo=FALSE, cache=TRUE}
par(mfrow=c(1,3)) 

#news.trigramfrec <- row_sums(news.tdm3, na.rm = TRUE)
#blog.trigramfrec <- row_sums(blog.tdm3, na.rm = TRUE)
#twitter.trigramfrec <- row_sums(twitter.tdm3, na.rm = TRUE)

hist(news.trigramfrec
     , breaks = 1000, xlim = c(0,25)
     , main = "News term frecuency", xlab = "Term's frecuency")
hist(blog.trigramfrec
     , breaks = 100, xlim = c(0,500)
     , main = "Blog term frecuency", xlab = "Term's frecuency")
hist(twitter.trigramfrec
     , breaks = 100, xlim = c(0,500)
     , main = "Twitter term frecuency", xlab = "Term's frecuency")

```

```{r trigramsTop, echo=FALSE, cache=TRUE}
par(las=2) # make label text perpendicular to axis
par(mfrow=c(1,3))
par(mar=c(8,10,1,2))

barplot(news.trigramfrec[1:25]
        , names.arg=names(news.trigramfrec[1:25])
        , main="News - 25 most frecuent trigrams"
        , horiz=TRUE
        , cex.names=1.2)
barplot(blog.trigramfrec[1:25]
        , names.arg=names(blog.trigramfrec[1:25])
        , main="Blog - 25 most frecuent trigrams"
        , horiz=TRUE
        , cex.names=1.2)
barplot(twitter.trigramfrec[1:25]
        , names.arg=names(twitter.trigramfrec[1:25])
        , main="Twitter - 25 most frecuent trigrams"
        , horiz=TRUE
        , cex.names=1.2)
```

#Coverage
How many unique words do you need in a frequency sorted dictionary to cover 50% of all word instances in the language? 90%?

I show coverage at 50% and 90% for each source in the next table.

```{r coverage, echo=FALSE, cache=TRUE}

getCoverage <- function (freqs,coverage) {
  counts = 0
  freqAcum = sum(freqs)*coverage
  for (i in 1:length(freqs)){
    if (counts < freqAcum){
      counts = counts + freqs[i]
    }
    else {
      return(i)
    }
  }
}

news.tdm.tot <- row_sums(news.tdm, na.rm = TRUE)
blog.tdm.tot <- row_sums(blog.tdm, na.rm = TRUE)
twitter.tdm.tot <- row_sums(twitter.tdm, na.rm = TRUE)

coverage.table <- data.frame(File = c("news", "blogs", "twitter")
                             , Words = sapply(list(news.tdm.tot, blog.tdm.tot, twitter.tdm.tot), sum)
                             , UniqueWords = sapply(list(news.tdm.tot, blog.tdm.tot, twitter.tdm.tot), length)
                             , UniqueWords_C0.5 = sapply(list(news.tdm.tot, blog.tdm.tot, twitter.tdm.tot), getCoverage, coverage=0.5)
                             , UniqueWords_C0.9 = sapply(list(news.tdm.tot, blog.tdm.tot, twitter.tdm.tot), getCoverage, coverage=0.9))

```
```{r echo=FALSE}
knitr::kable(coverage.table, caption = "Coverage")
```

#Foreign languages
How do you evaluate how many of the words come from foreign languages?.

* Check low frecuency words in a dictionary of language of corpus.
* Use of package to this propouse like cld of Google Chrome.

#Increase the coverage
Can you think of a way to increase the coverage -- identifying words that may not be in the corpora or using a smaller number of words in the dictionary to cover the same number of phrases?.

* Use of synonyms for low frecuency words to reduce size of dictionary.
* Have a dictionary for context.

#Conclusiones
* Size of corpus is a problem to processing, overall of memory, thus make this process in parallel allows manage more data. Size of corpus is not reflect to size of dictionary, process corpus in cluster but use the dictionaries in app.
* Most of the grams are low frecuencies, it generate big dictionary with low probability of use.
* With N (in n-gram) high, size of dictionary is size but exists a lot of grams with low probability of use.
* Remove stopWords is better for size of dictionary but remove a lot of probable ngrams (especially when n>1).


#Next steps to app
* Find the "best" máx. N to n-grams to maximize relation size/performance.
* Build a model to predict next word only with n-gram for the number of words written.
* Improve model with Katz Back-Off strategy.
* Improve model with synonymous.
* Choose context (source) to predict.

#Wordclouds

##News

```{r newsWordCloud, echo=FALSE, cache=TRUE, warning=FALSE}
par(mfrow=c(1,3))
par(mar=c(0,0,0,0))
wordcloud(names(news.wordfrec[3:50]),news.wordfrec[3:50],max.words=30,colors=brewer.pal(8,"Dark2"), fixed.asp = TRUE)
wordcloud(names(news.bigramfrec[3:50]),news.bigramfrec[3:50],max.words=30,colors=brewer.pal(8,"Dark2"), fixed.asp = TRUE)
wordcloud(names(news.trigramfrec[3:50]),news.trigramfrec[3:50],max.words=30,colors=brewer.pal(8,"Dark2"), fixed.asp = TRUE)
```

##Blog

```{r blogWordCloud, echo=FALSE, cache=TRUE, warning=FALSE}
par(mfrow=c(1,3))
par(mar=c(0,0,0,0))
wordcloud(names(blog.wordfrec[3:50]),blog.wordfrec[3:50],max.words=30,colors=brewer.pal(8,"Dark2"), fixed.asp = TRUE)
wordcloud(names(blog.bigramfrec[3:50]),blog.bigramfrec[3:50],max.words=30,colors=brewer.pal(8,"Dark2"), fixed.asp = TRUE)
wordcloud(names(blog.trigramfrec[3:50]),blog.trigramfrec[3:50],max.words=30,colors=brewer.pal(8,"Dark2"), fixed.asp = TRUE)
```

##Twitter

```{r twitterWordCloud, echo=FALSE, cache=TRUE, warning=FALSE}
par(mfrow=c(1,3))
par(mar=c(0,0,0,0))
wordcloud(names(twitter.wordfrec[3:50]),twitter.wordfrec[3:50],max.words=30,colors=brewer.pal(8,"Dark2"), fixed.asp = TRUE)
wordcloud(names(twitter.bigramfrec[3:50]),twitter.bigramfrec[3:50],max.words=30,colors=brewer.pal(8,"Dark2"), fixed.asp = TRUE)
wordcloud(names(twitter.trigramfrec[3:50]),twitter.trigramfrec[3:50],max.words=30,colors=brewer.pal(8,"Dark2"), fixed.asp = TRUE)
```


#Appendice


```{r ref.label='libraries', eval=FALSE, echo=TRUE}
```

```{r ref.label='variables', eval=FALSE, echo=TRUE}
```

```{r ref.label='download', eval=FALSE, echo=TRUE}
```

```{r ref.label='fileStats', eval=FALSE, echo=TRUE}
```

```{r ref.label='fileSample', eval=FALSE, echo=TRUE}
```

```{r ref.label='corpus', eval=FALSE, echo=TRUE}
```

```{r ref.label='unigrams', eval=FALSE, echo=TRUE}
```

```{r ref.label='unigramsHist', eval=FALSE, echo=TRUE}
```

```{r ref.label='unigramsTop', eval=FALSE, echo=TRUE}
```

```{r ref.label='bigrams', eval=FALSE, echo=TRUE}
```

```{r ref.label='bigramsHist', eval=FALSE, echo=TRUE}
```

```{r ref.label='bigramsTop', eval=FALSE, echo=TRUE}
```

```{r ref.label='trigrams', eval=FALSE, echo=TRUE}
```

```{r ref.label='bigramsHist', eval=FALSE, echo=TRUE}
```

```{r ref.label='trigramsTop', eval=FALSE, echo=TRUE}
```

```{r ref.label='coverage', eval=FALSE, echo=TRUE}
```

```{r ref.label='newsWordCloud', eval=FALSE, echo=TRUE}
```

```{r ref.label='blogWordCloud', eval=FALSE, echo=TRUE}
```

```{r ref.label='twitterWordCloud', eval=FALSE, echo=TRUE}
```
